```shell
spark-submit \
--name tianyafu_spark_test \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
--verbose \
/home/admin/app/spark/examples/jars/spark-examples_2.12-2.4.6.jar \
3

SparkSubmit
CoarseGrainedExecutorBackend
CoarseGrainedExecutorBackend
ExecutorLauncher

spark-submit \
--name tianyafu_spark_test \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
--verbose \
/home/admin/app/spark/examples/jars/spark-examples_2.12-2.4.6.jar \
3

SparkSubmit
CoarseGrainedExecutorBackend
CoarseGrainedExecutorBackend
ApplicationMaster
```
## 源码
```
SparkSubmit.main{
	val submit = new SparkSubmit()
    submit.doSubmit(args){
		val appArgs = parseArguments(args){
			mergeDefaultSparkProperties(){
				defaultSparkProperties{
					
				}
			}
			ignoreNonSparkProperties()
			loadEnvironmentArguments(){
				action = Option(action).getOrElse(SUBMIT)
			}
		}
		
		submit(appArgs, uninitLog){
			doRunMain{
				runMain(args, uninitLog){
					prepareSubmitEnvironment(args){
						if (deployMode == CLIENT) {
							childMainClass = args.mainClass
						}
						if (isYarnCluster) {
							childMainClass = "org.apache.spark.deploy.yarn.YarnClusterApplication"
						}
					}
					var mainClass: Class[_] = Utils.classForName(childMainClass)
					val app: SparkApplication = if (classOf[SparkApplication].isAssignableFrom(mainClass)) {
						mainClass.newInstance().asInstanceOf[SparkApplication]					
					}
					app.start(childArgs.toArray, sparkConf){
						new Client(new ClientArguments(args), conf).run(){
							this.appId = submitApplication(){
								val containerContext = createContainerLaunchContext(newAppResponse){
									val launchEnv = setupLaunchEnv(appStagingDirPath, pySparkArchives)
									val javaOpts = ListBuffer[String]()
									javaOpts += "-Xmx" + amMemory + "m"
									val amClass =
											  if (isClusterMode) {
												Utils.classForName("org.apache.spark.deploy.yarn.ApplicationMaster").getName
											  } else {
												Utils.classForName("org.apache.spark.deploy.yarn.ExecutorLauncher").getName
											  }
									val commands = prefixEnv ++
												  Seq(Environment.JAVA_HOME.$$() + "/bin/java", "-server") ++
												  javaOpts ++ amArgs ++
												  Seq(
													"1>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout",
													"2>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr")
								}
								val appContext = createApplicationSubmissionContext(newApp, containerContext)
							}
						}
					}
			}
		}
	}
}


ApplicationMaster.main(){
	val amArgs = new ApplicationMasterArguments(args)
    master = new ApplicationMaster(amArgs)
    master.run(){
		runImpl(){
			val priority = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY - 1
			ShutdownHookManager.addShutdownHook(priority) { () =>}
			
			if (isClusterMode) {
				runDriver(){
					userClassThread = startUserApplication(){
						val mainMethod = userClassLoader.loadClass(args.userClass).getMethod("main", classOf[Array[String]])
						val userThread = new Thread {
							override def run() {
								mainMethod.invoke(null, userArgs.toArray)
								finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
							}
						}
						userThread.setName("Driver")
						userThread.start()
					}
					
					val sc = ThreadUtils.awaitResult(sparkContextPromise.future,Duration(totalWaitTime, TimeUnit.MILLISECONDS))
					
					registerAM(host, port, userConf, sc.ui.map(_.webUrl)){
						client.register(host, port, yarnConf, _sparkConf, uiAddress, historyAddress)
					}
					
					val driverRef = rpcEnv.setupEndpointRef(RpcAddress(host, port),YarnSchedulerBackend.ENDPOINT_NAME)
					createAllocator(driverRef, userConf){
						allocator.allocateResources(){
							handleAllocatedContainers(allocatedContainers.asScala){
								runAllocatedContainers(containersToUse){
									for (container <- containersToUse) {
									launcherPool.execute(new Runnable {
											override def run(): Unit = {
											  try {
												new ExecutorRunnable(
												  Some(container),
												  conf,
												  sparkConf,
												  driverUrl,
												  executorId,
												  executorHostname,
												  executorMemory,
												  executorCores,
												  appAttemptId.getApplicationId.toString,
												  securityMgr,
												  localResources
												).run(){
													startContainer(){
														val commands = prepareCommand(){
															val commands = prefixEnv ++
															  Seq(Environment.JAVA_HOME.$$() + "/bin/java", "-server") ++
															  javaOpts ++
															  Seq("org.apache.spark.executor.CoarseGrainedExecutorBackend",
																"--driver-url", masterAddress,
																"--executor-id", executorId,
																"--hostname", hostname,
																"--cores", executorCores.toString,
																"--app-id", appId) ++
															  userClassPath ++
															  Seq(
																s"1>${ApplicationConstants.LOG_DIR_EXPANSION_VAR}/stdout",
																s"2>${ApplicationConstants.LOG_DIR_EXPANSION_VAR}/stderr")
														}
														
														nmClient.startContainer(container.get, ctx){
															
														}
													}
												}
											  }
											}
										  })
								}
							}
						}
					}
				}
			} else {
				runExecutorLauncher()
			}
		}
	}

}




SparkContext{
	private var _schedulerBackend: SchedulerBackend = _
    private var _taskScheduler: TaskScheduler = _
    private var _heartbeatReceiver: RpcEndpointRef = _
    @volatile private var _dagScheduler: DAGScheduler = _
	
	
	val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode){
		val scheduler = cm.createTaskScheduler(sc, masterUrl){
			sc.deployMode match {
			  case "cluster" => new YarnClusterScheduler(sc)
			  case "client" => new YarnScheduler(sc)
			}
		}
        val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler){
			sc.deployMode match {
			  case "cluster" =>
				new YarnClusterSchedulerBackend(scheduler.asInstanceOf[TaskSchedulerImpl], sc)
			  case "client" =>
				new YarnClientSchedulerBackend(scheduler.asInstanceOf[TaskSchedulerImpl], sc)
			}
		}
        cm.initialize(scheduler, backend){
			scheduler.asInstanceOf[TaskSchedulerImpl].initialize(backend){
				schedulableBuilder = {
				  schedulingMode match {
					case SchedulingMode.FIFO =>
					  new FIFOSchedulableBuilder(rootPool)
					case SchedulingMode.FAIR =>
					  new FairSchedulableBuilder(rootPool, conf)
				  }
				}
				schedulableBuilder.buildPools()
			}
		}
        backend, scheduler)
	}
	_dagScheduler = new DAGScheduler(this)
	_taskScheduler.start(){
		backend.start(){
			driverEndpoint = createDriverEndpointRef(properties){
				createDriverEndpoint(properties){
					new DriverEndpoint(rpcEnv, properties){
						onStart(){
							Option(self).foreach(_.send(ReviveOffers)){
								case ReviveOffers =>
									makeOffers()
							}
						}
					}
				}
			}
		}
	}



}



CoarseGrainedExecutorBackend.main(){
	run(driverUrl, executorId, hostname, cores, appId, workerUrl, userClassPath){
		val driver = fetcher.setupEndpointRefByURI(driverUrl)
		val env = SparkEnv.createExecutorEnv(driverConf, executorId, hostname, cores, cfg.ioEncryptionKey, isLocal = false){
			val env = create(
			  conf,
			  executorId,
			  hostname,
			  hostname,
			  None,
			  isLocal,
			  numCores,
			  ioEncryptionKey
			){
				val serializer = instantiateClassFromConf[Serializer]("spark.serializer", "org.apache.spark.serializer.JavaSerializer")
				val broadcastManager = new BroadcastManager(isDriver, conf, securityManager)
				val mapOutputTracker = if (isDriver) {
				  new MapOutputTrackerMaster(conf, broadcastManager, isLocal)
				} else {
				  new MapOutputTrackerWorker(conf)
				}
				val shortShuffleMgrNames = Map(
				  "sort" -> classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName,
				  "tungsten-sort" -> classOf[org.apache.spark.shuffle.sort.SortShuffleManager].getName)
				val shuffleMgrName = conf.get("spark.shuffle.manager", "sort")
				
				val memoryManager: MemoryManager =
				  if (useLegacyMemoryManager) {
					new StaticMemoryManager(conf, numUsableCores)
				  } else {
					UnifiedMemoryManager(conf, numUsableCores)
				  }
				
				val blockManagerMaster = new BlockManagerMaster(registerOrLookupEndpoint(
				  BlockManagerMaster.DRIVER_ENDPOINT_NAME,
				  new BlockManagerMasterEndpoint(rpcEnv, isLocal, conf, listenerBus)),
				  conf, isDriver)

				// NB: blockManager is not valid until initialize() is called later.
				val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster,
				  serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,
				  blockTransferService, securityManager, numUsableCores)
			}
		}
		
		new CoarseGrainedExecutorBackend(env.rpcEnv, driverUrl, executorId, hostname, cores, userClassPath, env){
			ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls)){
				DriverEndpoint.receiveAndReply{
					executorRef.send(RegisteredExecutor){
						CoarseGrainedExecutorBackend.receive{
							executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false)
						}
					
					}
				}
			}
		}
	}

}
```